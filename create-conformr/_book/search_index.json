[["index.html", "Creating the conformr R package 1 Introduction 1.1 Documentation Functions", " Creating the conformr R package Cynthia Huang 2024-07-01 1 Introduction Note: If you are working in RStudio, you can simply press “Knit” to render this bookdown (and open _book/index.html to see the result). More generally, in a console you can run the following: litr::render(&quot;create-conformr/index.Rmd&quot;, output_format = litr::litr_gitbook()) 1.1 Documentation Functions These functions are used to generate plots and other explanatory assets. They should not really live here. To convert a panel map into a Matrix # Convert an incidence table from long form to an incidence matrix inc_long_to_mtx &lt;- function(inc_long, to, weights){ inc_wide &lt;- inc_long |&gt; tidyr::pivot_wider(names_from = {{to}}, values_from = {{weights}}) inc_mtx &lt;- as.matrix(inc_wide[,-1]) dimnames(inc_mtx)[[1]] &lt;- inc_wide[,1, drop=TRUE] return(inc_mtx) } To plot an incidence matrix (without weights): plt_inc_long_mtx &lt;- function(inc_long, to, from, weights) { gg &lt;- inc_long |&gt; dplyr::mutate(src_case = dplyr::case_when( {{weights}}==1 ~ &quot;one-to-one&quot;, is.na({{weights}}) ~ &quot;none&quot;, {{weights}} &lt; 1 ~ &quot;one-to-many&quot;)) |&gt; ggplot(aes(x={{to}}, y={{from}})) + geom_tile(aes(fill=src_case), col=&quot;grey&quot;) + scale_y_discrete(limits=rev) + scale_x_discrete(position=&#39;top&#39;) + scale_fill_brewer() + coord_fixed() + labs(x = element_blank(), y = element_blank(), fill=&quot;source-to-target&quot;) + theme_minimal() return(gg) } To add labels for weights: geom_text(data = dplyr::filter(inc_long, !is.na(weight)), aes(label=round(weight, 2))) To plot dataframe as ggplot “matrix”: plt_df_mtx &lt;- function(x, cols_from, row_names){ x |&gt; dplyr::select({{row_names}}, {{cols_from}}) |&gt; tidyr::pivot_longer({{cols_from}}, names_to = &quot;var&quot;, values_to = &quot;value&quot;) |&gt; ggplot(aes(x=var, y={{row_names}})) + geom_tile(aes(fill=var), col=&quot;grey&quot;) + geom_text(aes(label=round(value, 2)), size=3) + scale_y_discrete(limits=rev) + scale_x_discrete(position=&#39;top&#39;) + scale_fill_brewer(palette=&quot;Greens&quot;) + coord_fixed() + labs(x = element_blank(), y = element_blank()) + theme_minimal() + theme(legend.position=&quot;none&quot;) } To plot a sigmoid plot of a panel map: library(ggbump) library(cowplot) library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union library(ggplot2) # Plot an incidence table (expanded panel map) as a ggplot sigmoid plot plt_pm_sigmoid &lt;- function(pm, from, to, weights){ edges &lt;- pm |&gt; transmute(from = {{from}}, to = {{to}}, weighted = {{weights}}) ## calculate positions for nodes from_nodes &lt;- distinct(edges, from) |&gt; mutate(from_y = row_number()) to_nodes &lt;- distinct(edges, to) |&gt; mutate(to_y = row_number() - 1 + 0.5) ## generate df for ggplot df &lt;- edges |&gt; ## generate mapping type/case variables group_by(from) |&gt; mutate(n_dest = n()) |&gt; ungroup() |&gt; group_by(to) |&gt; mutate(n_origin = n(), min_weight = min(weighted)) |&gt; ungroup() |&gt; mutate(value_case = case_when(n_dest == 1 ~ &quot;one-to-one&quot;, n_dest &gt; 1 ~ &quot;one-to-many&quot;)) |&gt; left_join(tribble(~value_case, ~line_type, ~font_type, &quot;one-to-one&quot;, &quot;solid&quot;, &quot;bold&quot;, &quot;one-to-many&quot;, &quot;dashed&quot;, &quot;italic&quot;), by = &quot;value_case&quot;) |&gt; mutate(from_case = case_when(n_origin == 1 ~ &quot;one-from-one&quot;, n_origin &gt; 1 ~ &quot;one-from-many&quot;, n_origin &lt; 1 ~ &quot;ERROR! origin codes &lt; 1&quot;), dest_case = case_when(min_weight &lt; 1 ~ &quot;contains split&quot;, min_weight == 1 ~ &quot;aggregation only&quot;, min_weight &gt; 1 ~ &quot;ERROR! weight &gt; 1&quot;) ) |&gt; ## add y-coordinates left_join(from_nodes, by = &quot;from&quot;) |&gt; left_join(to_nodes, by = &quot;to&quot;) |&gt; ## add x-coordinates mutate(from_x = 0, to_x = 5) |&gt; ## give each from-out instruction a unique id mutate(idx = row_number()) plt_uw &lt;- df |&gt; ggplot(aes(x = from_x, xend = to_x, y = from_y, yend = to_y, group = idx)) + ## edges as sigmoid curves with line type geom_sigmoid(aes(linetype = I(line_type))) + # to/from nodes scale_y_reverse() + geom_text(aes(x = from_x - 0.5, label=from, fontface=I(font_type))) + geom_label(aes(x = to_x + 0.5, y = to_y, label=to, fill = dest_case)) + # edge labels geom_label(data = filter(df, value_case == &quot;one-to-many&quot;), aes(x = (((from_x + to_x) / 2) + to_x) / 2, y = to_y, label = weighted)) + geom_label(data = filter(df, value_case == &quot;one-to-one&quot;), aes(x = (from_x + to_x) / 4, y = from_y, label = weighted)) + # theme cowplot::theme_minimal_grid(font_size = 14, line_size = 0) + theme(legend.position = &quot;bottom&quot;, panel.grid.major = element_blank(), axis.text.y = element_blank(), axis.text.x = element_blank(), plot.background = element_rect(fill = &quot;white&quot;)) + labs(x = NULL, y = NULL, fill = &quot;target-from-sources&quot;) return(plt_uw) } "],["package-design-notes.html", "2 Package Design Notes 2.1 Example Dataset Designs 2.2 Existing Implementations and Approaches 2.3 Existing Crosswalk Packages", " 2 Package Design Notes 2.1 Example Dataset Designs This section collects and/or defines example datasets, concordances and transformations from different domains. Types of crosswalks/concordances include: Classification changes - HS, ISIC etc. Geographic (administrative/survey) area changes – Queensland Government Statistician’s Office 2.1.1 INDSTAT across ISIC standards Initial reproducible approach: https://cynthiahqy.github.io/indstat-TPP/ Demo rewrite of first transformation step using old version of {conformr}: https://github.com/cynthiahqy/conformr-indstat 2.1.2 WTO/ComTrade Data across Harmonised System versions 2.1.3 ABS labor force statistics across ANZSCO 2.2 Existing Implementations and Approaches Collection of current approaches to harmonisation 2.2.1 hadley_data-fuel-economy From @wickham2014 and tidy-data vignette: A more complicated situation occurs when the dataset structure changes over time. For example, the datasets may contain different variables, the same variables with different names, different file formats, or different conventions for missing values. This may require you to tidy each file to individually (or, if you’re lucky, in small groups) and then combine them once tidied. An example of this type of tidying is illustrated in https://github.com/hadley/data-fuel-economy, which shows the tidying of epa fuel economy data for over 50,000 cars from 1978 to 2008. The raw data is available online, but each year is stored in a separate file and there are four major formats with many minor variations, making tidying this dataset a considerable challenge. 2.2.2 schott_stata-trade-concordance See: https://sompks4.github.io/sub_data.html 2.2.3 kolczynska_trust-in-institutions Link to code for crosswalk based harmonisation: https://osf.io/qt2eb/ From @kolczynska2022: Ex-post harmonization of survey data creates new opportunities for research by extending the geographical and/or time coverage of analyses. Researchers increasingly combine data from different survey projects to analyze them as a single dataset, and while teams engaged in data harmonization continue to expand the information they provide to end users, there are still no commonly agreed standards for the documentation of data processing. Existing harmonization project typically opt for recode scripts that are generally hard to read, modify, and reuse, although some projects make efforts to facilitate verification and reproduction. This paper describes an alternative procedure and a set of simple tools for the exploration, recoding, and documentation of harmonization of survey data, relying on crosswalks. The presented tools are flexible and software-agnostic. The illustrative example uses the programming language R and spreadsheets—both common software choices among social scientists. Harmonization of variables on trust in institutions from four major cross-national survey projects serves as an illustration of the proposed workflow and of opportunities harmonization creates. 2.3 Existing Crosswalk Packages A collection of R packages that (I think) offer special cases or components of the conformr workflow 2.3.1 countrycode_look-up-tables 2.3.2 concordance_look-up-tables 2.3.3 debkeepr_non-decimal-currencies By Jesse Sadler: The debkeepr package provides an interface for working with non-decimal currencies that use tripartite or tetrapartite systems such as that of pounds, shillings, and pence. debkeepr makes it easier to perform arithmetic operations on non-decimal values and facilitates the analysis and visualization of larger sets of non-decimal values such as those found in historical account books. This is accomplished through the implementation of the deb_lsd,deb_tetra, and deb_decimal vector types, which are based on the infrastructure provided by the vctrs package. deb_lsd, deb_tetra, and deb_decimal vectors possess additional metadata to allow them to behave like numeric vectors in many circumstances, while also conforming to the workings of non-decimal currencies. "],["package-setup.html", "3 Package setup 3.1 Dependencies 3.2 Utilities", " 3 Package setup Every R package needs a DESCRIPTION file, so we start by specifying this information: usethis::create_package( path = &quot;.&quot;, fields = list( Package = params$package_name, Version = &quot;0.0.0.9006&quot;, Title = &quot;An opinionated toolkit for harmonising data&quot;, Description = &quot;conformr provides tools and workflows for converting data between classification standards, and correcting discrepancies in reported and calculated statistics.&quot;, `Authors@R` = c( person(given = &quot;Cynthia&quot;, family = &quot;Huang&quot;, email = &quot;cynthia@gmail.com&quot;, role = c(&quot;aut&quot;, &quot;cre&quot;)), person(given = &quot;Laura&quot;, family = &quot;Puzzello&quot;, role = c(&quot;aut&quot;, &quot;fnd&quot;)) ), `Config/testthat/edition` = 3 ) ) usethis::use_mit_license(copyright_holder = &quot;C. Huang&quot;) Although it’s not required, it can be nice to add some package-level documentation. This is what will show up when someone types package?&lt;your-package-name&gt; in the console. #&#39; Validated Data Harmonisation #&#39; #&#39; Provides an opinionated toolkit for harmonising data from related nomenclature or classifications into a single consistent validated dataset. Offers safe-guards against common but often subtle data loss or corruption mistakes. #&#39; #&#39; @docType package 3.1 Dependencies ## imports usethis::use_package(&quot;dplyr&quot;) ## ✔ Adding &#39;dplyr&#39; to Imports field in DESCRIPTION ## • Refer to functions with `dplyr::fun()` usethis::use_package(&quot;rlang&quot;) ## ✔ Adding &#39;rlang&#39; to Imports field in DESCRIPTION ## • Refer to functions with `rlang::fun()` usethis::use_package(&quot;cli&quot;) ## ✔ Adding &#39;cli&#39; to Imports field in DESCRIPTION ## • Refer to functions with `cli::fun()` ## suggests 3.2 Utilities These functions are not exported. #&#39; Defaults for NULL values #&#39; `%||%` &lt;- function(x, y) if (is.null(x)) y else x "],["core-functions.html", "4 Core Functions 4.1 Setup Testing Data 4.2 Valid Transformation Conditions 4.3 Use Panel Map on Data", " 4 Core Functions 4.1 Setup Testing Data Define a toy example to use in development: ## correspondence/concordance table codes_BA &lt;- dplyr::tribble(~ std_B, ~ std_A, &quot;A1&quot;, &quot;x1111&quot;, # one-to-one &quot;B2&quot;, &quot;x2222&quot;, # many-to-one &quot;B2&quot;, &quot;x3333&quot;, &quot;C3&quot;, &quot;x4444&quot;, # one-to-many (4) &quot;C4&quot;, &quot;x4444&quot;, &quot;C4&quot;, &quot;x6666&quot;, # many-to-many &quot;C5&quot;, &quot;x4444&quot;, &quot;C6&quot;, &quot;x4444&quot;, &quot;C7&quot;, &quot;x5555&quot;, # one-to-many (3) &quot;C8&quot;, &quot;x5555&quot;, ) ## panel_map weights_BA &lt;- codes_BA |&gt; dplyr::distinct(std_B, std_A) |&gt; dplyr::group_by(std_A) |&gt; dplyr::mutate(n_dest = dplyr::n(), weight = 1 / n_dest) |&gt; dplyr::ungroup() pm_BA &lt;- weights_BA |&gt; dplyr::select(std_B, std_A, weight) Write this data into an internal list for testing purposes. equal_pm &lt;- list(&quot;codes_BA&quot; = codes_BA, &quot;weights_BA&quot; = weights_BA, &quot;pm_BA&quot; = pm_BA) We can visualise a panel map as the addition of weights to the concordance: library(ggplot2) inc_long &lt;- tidyr::expand(codes_BA, std_A, std_B) |&gt; dplyr::left_join(pm_BA, by = c(&quot;std_A&quot;, &quot;std_B&quot;)) |&gt; dplyr::transmute(to = std_B, from = std_A, weight = weight) gg_inc_mtx &lt;- inc_long |&gt; plt_inc_long_mtx(to, from, weight) + ggtitle(&quot;Concordance as Incidence Matrix&quot;) gg_pm_mtx &lt;- gg_inc_mtx + geom_text(data = dplyr::filter(inc_long, !is.na(weight)), aes(label=round(weight, 2))) + ggtitle(&quot;adding equal weights for Valid Panel Map&quot;) gg_inc_mtx gg_pm_mtx 4.2 Valid Transformation Conditions 4.2.1 Complete Mapping Weights A valid panel map is an mapping from source to target nomenclatures which when applied to suitably dimensioned source data, transforms that data into the target nomenclature without creation or loss of value (beyond floating point rounding). This can also be thought of as a condition whereby the sum total of a variable remains the same before and after the transformation. The following condition is necessary and sufficient for a set of Source Codes, Target Codes and Mapping Weights to be a valid panel map: The sum of all Mapping Weights associated with any given Source Code totals to 1 To demonstrate, let us generate some source data: ## generate some data set.seed(1832) std_A_codes &lt;- unique(codes_BA$std_A) (data_A &lt;- dplyr::tibble(std_A = std_A_codes, A_100 = 100, A_prod = round(abs(rnorm(length(std_A_codes)) * 10000),2) ) ) ## # A tibble: 6 × 3 ## std_A A_100 A_prod ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 x1111 100 15275. ## 2 x2222 100 7432. ## 3 x3333 100 1970. ## 4 x4444 100 837. ## 5 x6666 100 9976. ## 6 x5555 100 1217. Create more testing data. equal_pm$data_A &lt;- data_A |&gt; dplyr::select(std_A, A_100) Now let’s switch to using the matrix representation of panel maps: Let \\(\\bf{C}\\) be a \\(n \\times m\\) matrix showing the incidence between two disjoint sets (inc_mtx), and let \\(\\bf{X}\\) be the source variables (x_mtx) requiring transformation. Then, the transformed data is \\(\\bf{Z} = \\bf{C&#39;X}\\): ## incidence matrix inc_mtx &lt;- inc_long |&gt; tidyr::replace_na(list(weight=0)) |&gt; inc_long_to_mtx(to, weight) ## source data matrix x_mtx &lt;- as.matrix(data_A[,-1]) dimnames(x_mtx)[[1]] &lt;- std_A_codes ## transformed data z_mtx &lt;- t(inc_mtx) %*% x_mtx round(t(inc_mtx), 2) ## x1111 x2222 x3333 x4444 x5555 x6666 ## A1 1 0 0 0.00 0.0 0 ## B2 0 1 1 0.00 0.0 0 ## C3 0 0 0 0.25 0.0 0 ## C4 0 0 0 0.25 0.0 1 ## C5 0 0 0 0.25 0.0 0 ## C6 0 0 0 0.25 0.0 0 ## C7 0 0 0 0.00 0.5 0 ## C8 0 0 0 0.00 0.5 0 print(x_mtx) ## A_100 A_prod ## x1111 100 15274.93 ## x2222 100 7431.98 ## x3333 100 1970.36 ## x4444 100 836.72 ## x6666 100 9976.27 ## x5555 100 1216.67 print(z_mtx) ## A_100 A_prod ## A1 100 15274.930 ## B2 200 9402.340 ## C3 25 209.180 ## C4 125 1425.850 ## C5 25 209.180 ## C6 25 209.180 ## C7 50 4988.135 ## C8 50 4988.135 Notice that the sum total of A_100 is the same before and after the transformation. colSums(x_mtx) ## A_100 A_prod ## 600.00 36706.93 colSums(z_mtx) ## A_100 A_prod ## 600.00 36706.93 Now, let’s edit the panel map such that the weights no longer sum to one: ## edit weights bad_pm &lt;- pm_BA |&gt; dplyr::mutate(weight = dplyr::case_when( weight == 1 ~ weight, weight &lt; 0.5 ~ weight - 0.03, weight &gt;= 0.5 ~ weight + 0.01, T ~ weight)) ## incidence matrix bad_mtx &lt;- bad_pm |&gt; inc_long_to_mtx(std_B, weight) bad_mtx[is.na(bad_mtx)] &lt;- 0 ## transform data badly bad_z &lt;- t(bad_mtx) %*% x_mtx Notice what happens when we apply the transformation: round(t(bad_mtx), 2) ## x1111 x2222 x3333 x4444 x6666 x5555 ## A1 1 0 0 0.00 0 0.00 ## B2 0 1 1 0.00 0 0.00 ## C3 0 0 0 0.22 0 0.00 ## C4 0 0 0 0.22 1 0.00 ## C5 0 0 0 0.22 0 0.00 ## C6 0 0 0 0.22 0 0.00 ## C7 0 0 0 0.00 0 0.51 ## C8 0 0 0 0.00 0 0.51 print(x_mtx) ## A_100 A_prod ## x1111 100 15274.93 ## x2222 100 7431.98 ## x3333 100 1970.36 ## x4444 100 836.72 ## x6666 100 9976.27 ## x5555 100 1216.67 print(bad_z) ## A_100 A_prod ## A1 100 15274.9300 ## B2 200 9402.3400 ## C3 22 184.0784 ## C4 122 10160.3484 ## C5 22 184.0784 ## C6 22 184.0784 ## C7 51 620.5017 ## C8 51 620.5017 Notice that the sum totals are no longer the same before and after the transformation: colSums(x_mtx) ## A_100 A_prod ## 600.00 36706.93 colSums(bad_z) ## A_100 A_prod ## 590.00 36630.86 Hence, the validity condition can also be expressed as follows: &gt; A given incidence matrix \\(\\bf{K}\\) with dimensions \\(n \\times m\\) is a valid panel map if and only if \\(\\bf{K}\\boldsymbol{1} = \\boldsymbol{1}\\) where \\(\\boldsymbol{1}\\) is a unit vector of length \\(m\\): ones &lt;- rep_len(1, ncol(inc_mtx)) round(inc_mtx, 2) ## A1 B2 C3 C4 C5 C6 C7 C8 ## x1111 1 0 0.00 0.00 0.00 0.00 0.0 0.0 ## x2222 0 1 0.00 0.00 0.00 0.00 0.0 0.0 ## x3333 0 1 0.00 0.00 0.00 0.00 0.0 0.0 ## x4444 0 0 0.25 0.25 0.25 0.25 0.0 0.0 ## x5555 0 0 0.00 0.00 0.00 0.00 0.5 0.5 ## x6666 0 0 0.00 1.00 0.00 0.00 0.0 0.0 inc_mtx %*% ones ## [,1] ## x1111 1 ## x2222 1 ## x3333 1 ## x4444 1 ## x5555 1 ## x6666 1 4.2.1.1 Functions Internal switching function for flow control and error messages #&#39; Flag Bad Mapping Weights #&#39; has_bad_weights &lt;- function(df, code_in, code_out, weights){ bad_rows &lt;- df |&gt; dplyr::group_by({{code_in}}) |&gt; dplyr::summarise(total = sum({{weights}}), weights = paste({{weights}}, collapse=&quot;,&quot;)) |&gt; dplyr::filter(total != 1) is_bad &lt;- !(nrow(bad_rows) == 0) result &lt;- list(fail = is_bad, table = bad_rows) return(result) } This function checks if the panel map has valid weights and returns the panel map if it does. It can be used to validate a panel map after editing or modifications. For example: ## prepare panel map new_pm &lt;- old_pm |&gt; mutate() |&gt; filter() |&gt; check_pm_weights(code_in, code_out, weights) #&#39; Check panel map weights are valid #&#39; #&#39; Checks if `code_in`, `code_out` and `weights` columns of data frame forms a valid panel map. #&#39; #&#39; @param df Data Frame containing weighted links `weights` between `code_in` and `code_out`. #&#39; @param code_in Variable in `code_dict` containing source codes to convert from. #&#39; @param code_out Variable in `code_dict` containing destination codes to convert to. #&#39; @param weights Column containing weights for transforming values from `code_in` to `code_out` #&#39; #&#39; @exports #&#39; #&#39; @returns The original data frame if the check is passed and an error if not. check_weights &lt;- function(df, code_in, code_out, weights){ has_result &lt;- has_bad_weights(df, {{code_in}}, {{code_out}}, {{weights}}) if (has_result$fail){ cli::cli_abort(c( &quot;{.var weights} for each {.var code_in} must sum to 1&quot;, &quot;&quot; ), class=&quot;invalid_weights&quot; ) } else { return(df) } } 4.2.1.2 Tests flag function returns expected output check function works as expected: returns informative error message returns unchanged panel map Add testing data equal_pm$bad_weights &lt;- equal_pm$pm_BA |&gt; dplyr::mutate(weight = dplyr::case_when( weight == 1 ~ weight, weight &lt; 0.5 ~ weight - 0.03, weight &gt;= 0.5 ~ weight + 0.01, T ~ weight)) Write tests: testthat::test_that( &quot;has_bad_weights() returns correct flags&quot;, { # good weights testthat::expect_false( has_bad_weights(equal_pm$pm_BA, std_A, std_B, weight)$fail ) # bad weights testthat::expect_true( has_bad_weights(equal_pm$bad_weights, std_A, std_B, weight)$fail ) } ) ## Test passed testthat::test_that( &quot;check_weights() works as expected&quot;, { # good weights testthat::expect_identical( check_weights(equal_pm$pm_BA, std_A, std_B, weight), equal_pm$pm_BA) # bad weights testthat::expect_error( check_weights(equal_pm$bad_weights, std_A, std_B, weight), class=&quot;invalid_weights&quot; ) } ) ## Test passed 4.2.2 No Missing Data Values Except for a one-to-one transfer between classifications, there is no way for NA values in the Source Data to be preserved when transformed into the Target Classification. It doesn’t make sense to split NA into smaller parts, or to aggregate NA into a sum. Hence, any missing values need to be explicitly dealt with before applying a Panel Map transformation. Exactly how missing values should be treated will vary from dataset to dataset. This could involve replace the missing values with zeroes or some imputed values, or to remove them completely. pm_BA |&gt; plt_pm_sigmoid(from=std_A, to=std_B, weights = weight) + scale_fill_brewer(palette=&quot;RdPu&quot;, direction=-1) 4.2.2.1 Functions This function flags if the variables you want to transform have any missing values. #&#39; Flags NA in Source Data #&#39; has_missing &lt;- function(.data){ is_miss &lt;- .data |&gt; anyNA() result &lt;- list(fail=is_miss) return(result) } This function checks the dataframe for missing values, and returns the original dataframe or tells the user to fix the NAs in their data. The dataframe should already be subsetted to contain only the Source Code and Source Value columns: ## prepare data for transformation data_in &lt;- all_df |&gt; select(code_in, x1, x2) |&gt; check_missing() #&#39; Checks Source Data for Missing Values #&#39; #&#39; @inheritParams concord #&#39; #&#39; @export check_missing &lt;- function(data_in){ has_result &lt;- has_missing(data_in) if(has_result$fail){ cli::cli_abort( &quot;{.var data_in} should not have any NA&quot;, class=&quot;vals_na&quot; ) } else { return(data_in) } } 4.2.2.2 Tests Feed in data with missing values and expect: - TRUE flag - Error message Add testing data equal_pm$bad_data &lt;- equal_pm$data_A equal_pm$bad_data[1, 2] &lt;- NA testthat::test_that( &quot;has_missing() returns expected flags&quot;, { # good weights testthat::expect_false( has_missing(equal_pm$data_A)$fail ) # bad weights testthat::expect_true( has_missing(equal_pm$bad_data)$fail ) } ) ## Test passed testthat::test_that( &quot;check_missing() works as expected&quot;, { ## good data testthat::expect_identical(check_missing(equal_pm$data_A), equal_pm$data_A) ## bad data testthat::expect_error(check_missing(equal_pm$bad_data), class = &quot;vals_na&quot;) } ) ## Test passed 4.2.3 Source Code Coverage A Panel Map must cover all Source Codes present in the Source Data. In other words, for a transformation to be valid, no Source Data should be left behind. gg_x_mtx &lt;- plt_df_mtx(data_A, A_100:A_prod, std_A) library(patchwork) ## ## Attaching package: &#39;patchwork&#39; ## The following object is masked from &#39;package:cowplot&#39;: ## ## align_plots gg_pm_mtx + guides(fill=&quot;none&quot;) + ggtitle(&quot;&quot;) + gg_x_mtx + scale_y_discrete(position=&quot;right&quot;, limits=rev) + patchwork::plot_annotation(title=&quot;Panel Map covers Source Data&quot;) ## Scale for y is already present. ## Adding another scale for y, which will replace the existing scale. gg_x_bad &lt;- data_A |&gt; dplyr::add_row(std_A = &quot;x7285!&quot;, A_100 = 100, A_prod = 3895.3) |&gt; plt_df_mtx(A_100:A_prod, std_A) gg_pm_bad &lt;- tidyr::expand_grid(from=c(NA), to=unique(codes_BA$std_B)) |&gt; dplyr::mutate(weight=NA) |&gt; bind_rows(inc_long) |&gt; plt_inc_long_mtx(to, from, weight) + geom_text(data = dplyr::filter(inc_long, !is.na(weight)), aes(label=round(weight, 2))) library(patchwork) gg_pm_bad + guides(fill=&quot;none&quot;) + ggtitle(&quot;&quot;) + gg_x_bad + scale_y_discrete(position=&quot;right&quot;, limits=rev) + scale_fill_brewer(palette=&quot;Purples&quot;) + patchwork::plot_annotation(title=&quot;Panel Map does not cover fully Source Data&quot;) ## Scale for y is already present. ## Adding another scale for y, which will replace the existing scale. ## Scale for fill is already present. ## Adding another scale for fill, which will replace the existing scale. Depending on how the transformation is implemented, coverage mismatches can result in both explicit and implicit/hidden errors. In particular, having conformable matrix dimensions is not sufficient to avoid corrupting data unless you check that the indices match. This is a common issue with using matrices for data wrangling, so this package implements transformations using database operations. 4.2.3.1 Functions Internal checking function – assumes .map is a valid map. Note this could be (quickly) checked for using a class condition discussed in GitHub issue #43 #&#39; Flag if data set is not completely cover by panel map #&#39; #&#39; @inheritParams use_panel_map #&#39; has_coverage &lt;- function(.data, .map, .from){ missing_links &lt;- .data |&gt; dplyr::select(tidyselect::all_of(.from)) |&gt; dplyr::distinct() |&gt; dplyr::anti_join(.map, by = .from) is_covered &lt;- (nrow(missing_links) == 0) results &lt;- list(fail=!is_covered, table=missing_links) return(results) } Error constructing function, also used in concord() #&#39; Check coverage of panel map over source data #&#39; #&#39; @inheritParams concord #&#39; @inheritParams use_panel_map #&#39; #&#39; @returns `data_in` if check is successful, throws error otherwise. #&#39; @examples #&#39; #&#39; /notrun{ #&#39; check_coverage(df, pm, &quot;std_A&quot;) #&#39; } #&#39; #&#39; check_coverage &lt;- function(data_in, pm, .from){ # call flag function has_result &lt;- has_coverage(data_in, pm, .from) # conditionals if(has_result$fail){ cli::cli_abort( &quot;{.var data_in$from_code} has values not covered by {.var pm$from_code}&quot;, class=&quot;not_covered&quot; ) } else { return(data_in) } } 4.2.3.2 Tests Add some more testing data equal_pm$data_extra &lt;- equal_pm$data_A |&gt; dplyr::add_row(std_A = &quot;x7777&quot;, A_100 = 100) testthat::test_that( &quot;has_coverage() returns expected flags&quot;, { ## complete coverage testthat::expect_false(has_coverage(equal_pm$data_A, equal_pm$pm_BA, &quot;std_A&quot;)$fail) ## incomplete coverage testthat::expect_true(has_coverage(equal_pm$data_extra, equal_pm$pm_BA, &quot;std_A&quot;)$fail) } ) ## Test passed testthat::test_that( &quot;check_coverage() works as expected&quot;, { ## complete coverage testthat::expect_identical(check_coverage(equal_pm$data_A, equal_pm$pm_BA, &quot;std_A&quot;), equal_pm$data_A) ## incomplete coverage testthat::expect_error(check_coverage(equal_pm$data_extra, equal_pm$pm_BA, &quot;std_A&quot;), class = &quot;not_covered&quot;) } ) ## Test passed 4.3 Use Panel Map on Data 4.3.1 Single Step Concordance 4.3.1.1 Stylized Code Assuming all the validity conditions are met, we want a simple and concise way to apply a panel map to data which looks something like: # --- prepare panel map -------------------- df_pm &lt;- read_csv(&quot;concordance-table.csv&quot;) |&gt; conformr::make_panel_map_equal(...) |&gt; conformr::validate_panel_map(...) # --- prepare data --------------------------- df_data_in &lt;- read_csv(&quot;your-source-data.csv&quot;) |&gt; conformr::validate_data_in(...) ## --- apply (valid) transformation ----------- conformr::concord( data_in = df_data_in, pm = df_pm, from_code = source, to_code = target, m_weights = weight, values_from = value_in, .suffix = &quot;_out&quot; ) Preparing a panel map and data for valid transformation could look like: ## --- prepare panel map -------------------- ## # by importing a manually encoded map df_pm &lt;- read_csv(&quot;your-panel-map.csv&quot;) # or creating one from a concordance table df_pm &lt;- read_csv(&quot;concordance-table.csv&quot;) |&gt; conformr::make_panel_map_equal( code_in = source, code_out = target, .weights_to = &quot;weight&quot;) ## --- prepare source data ------------------ ## # example using {dplyr}: df_data_in &lt;- read_csv(&quot;your-source-data.csv&quot;) |&gt; drop_na() |&gt; group_by(source) |&gt; summarise(value_in = sum(gdp)) ## --- apply (valid) transformation --------- ## conformr::concord( data_in = df_data_in, pm = df_pm, from_code = source, to_code = target, m_weights = weight, values_from = value_in, .suffix = &quot;_out&quot; ) 4.3.1.2 Warnings and Errors The concordance function should throw error when: panel map (pm) has invalid weights source data (data_in) column has missing values The concordance function should warn users about data prep?: multiple rows for a given code_in in data_in; should only have one set of value_in for each code_in 4.3.1.3 Functions This function takes a valid panel map and data with matching names for the Source Code columns and transforms the data to the Target Classification. Add informative error messages later: in_data_in &lt;- (str.vals %in% colnames(data_in)) if (!all(in_data_in)){ cli::cli_abort( &quot;{.code {names(dots)[!in_data_in]}} cannot be found in {.var data_in}&quot;, class = &quot;cols_not_found&quot;) } #&#39; Transform data from Source to Target classification using Panel Map #&#39; #&#39; Currently checks for valid Mapping weights, missing values, and coverage. #&#39; #&#39; @param data_in A Data Frame containing the values you want to transform #&#39; @param pm A Data Frame containing valid Mapping Weights between `from_code` and `to_code`. #&#39; @param from_code Variable containing Source Codes. Must be present in both `data_in` and `pm` #&#39; @param to_code Variable in `pm` containing Target Codes. #&#39; @param m_weights Variable in `pm` containing Mapping Weights. #&#39; @param values_from A vector of variables in `data_in` to be transformed. E.g. `c(var1, var2)` #&#39; @param .suffix An (optional) string appended to each `values_from` name to create column names for transformed values. #&#39; Defaults to `&quot;_out&quot;` #&#39; #&#39; @return #&#39; @export #&#39; #&#39; @examples #&#39; \\dontrun{ #&#39; concord(data_in = equal_pm$data_A, #&#39; pm = equal_pm$pm_BA, #&#39; from_code = std_A, #&#39; to_code = std_B, #&#39; m_weights = weight, #&#39; values_from = c(A_100), #&#39; .suffix = &quot;_out&quot;) #&#39; } #&#39; concord &lt;- function(data_in, pm, from_code, to_code, m_weights, values_from, .suffix=NULL){ ## defuse arugments str.to &lt;- rlang::as_string(rlang::enexpr(to_code)) str.from &lt;- rlang::as_string(rlang::enexpr(from_code)) ## check conditions pm |&gt; check_weights(code_in = {{from_code}}, code_out = {{to_code}}, weights = {{m_weights}}) subset_in &lt;- tryCatch( data_in |&gt; dplyr::select({{from_code}}, {{values_from}}), error = function(cnd) { cli::cli_abort( &quot;{.var from_code} or {.var values_from} could not be found in {.var data_in}&quot;, class = &quot;vals_not_found&quot;) } ) subset_in |&gt; check_missing() check_coverage(subset_in, pm, str.from) ## apply transformation # -- create suffix -- out_suffix &lt;- .suffix %||% paste0(&quot;_&quot;, str.to) join_by &lt;- str.from data_out &lt;- use_panel_map(.data = subset_in, .map = pm, .from = {{from_code}}, .to = {{to_code}}, .weights = {{m_weights}}, .vals = {{values_from}}, .suffix = out_suffix, .by = join_by) return(data_out) } Internal function without checks #&#39; Apply panel_map to data without checks #&#39; #&#39; A wrapper around a `{dplyr}` pipeline that takes a panel_map, #&#39; joins it with data, and transforms selected variables in that data according to #&#39; instructions in the panel map. Any groups in `data_in` are preserved. #&#39; #&#39; @param .data a Data Frame assumed to meet Source Data conditions #&#39; @param .map a Data Frame assumed to meet Panel Map conditions #&#39; #&#39; @return The output has the following properties: #&#39; * Groups are taken from `data_in` #&#39; use_panel_map &lt;- function(.data, .map, .from, .to, .weights, .vals, .suffix, .by){ # subset data for transformation data_in &lt;- .data %&gt;% dplyr::select({{.from}}, {{.vals}}) # merge map and data // use default by= argument map_join_data &lt;- dplyr::right_join(x = data_in, y = .map, by = .by) # apply transformation data_out &lt;- map_join_data %&gt;% dplyr::mutate(dplyr::across({{ .vals }}, ~ .x * {{ .weights }})) %&gt;% dplyr::group_by({{ .to }}, .add = TRUE) %&gt;% dplyr::summarise(dplyr::across({{ .vals }}, ~ sum(.x)), .groups = &quot;drop_last&quot;) # rename data_out &lt;- data_out %&gt;% dplyr::rename_with(., ~ paste0(.x, .suffix), .cols = {{.vals}}) return(data_out) } 4.3.1.4 Tests Define some test data: equal_pm$data_B &lt;- dplyr::right_join(x = equal_pm$data_A, y = equal_pm$pm_BA, by = &quot;std_A&quot;) |&gt; dplyr::mutate(A_100 = A_100 * weight) |&gt; dplyr::group_by(std_B, .add = TRUE) |&gt; dplyr::summarise(dplyr::across(c(A_100), ~ sum(.x), .names = &quot;{.col}_out&quot;), .groups = &quot;drop_last&quot;) Do the tests: testthat::test_that( &quot;use_panel_map() works as expected&quot;, { testthat::expect_identical( use_panel_map(.data = equal_pm$data_A, .map = equal_pm$pm_BA, .from = std_A, .to = std_B, .weights = weight, .vals = c(A_100), .suffix = &quot;_out&quot;, .by = &quot;std_A&quot;), equal_pm$data_B ) } ) ## Test passed testthat::test_that( &quot;concord() raises expected errors&quot;, { ## columns not in data_in testthat::expect_error(concord(data_in = equal_pm$data_A, pm = equal_pm$pm_BA, from_code = std_A, to_code = std_B, m_weights = weight, values_from = c(missing_col1, missing_col2) ), class=&quot;vals_not_found&quot;) ## missing values in data_in testthat::expect_error(concord(equal_pm$bad_data, equal_pm$pm_BA, std_A, std_B, weight, values_from = c(A_100) ), class=&quot;vals_na&quot; ) ## invalid weights are flagged testthat::expect_error(concord(equal_pm$data_A, equal_pm$bad_weights, std_A, std_B, weight, values_from = c(A_100) ), class=&quot;invalid_weights&quot; ) } ) ## Test passed testthat::test_that( &quot;concord() works as expected&quot;, { testthat::expect_identical(concord(data_in = equal_pm$data_A, pm = equal_pm$pm_BA, from_code = std_A, to_code = std_B, m_weights = weight, values_from = c(A_100), .suffix = &quot;_out&quot;), equal_pm$data_B ) } ) ## Test passed "],["package-datasets.html", "5 Package Datasets", " 5 Package Datasets Data used in vignettes or examples in the package. "],["validation-helper-functions.html", "6 Validation Helper Functions 6.1 Panel Map 6.2 Data Preparation for Concordance Transformation", " 6 Validation Helper Functions 6.1 Panel Map 6.1.1 Generate Equal Weights In the most simple case, to make a panel map, we need only a correspondence between a source nomenclature (std_A) and target nomenclature (std_B), which doesn’t have any duplicate rows. 6.1.1.1 Functions This is a helper function for making a valid Panel Map with equal Mapping Weights from a concordance table. #&#39; Helper to build equal split panel map #&#39; #&#39; Generate panel map using all *distinct* correspondences between two classifications. #&#39; #&#39; @param code_dict Data frame containing correspondence between source and destination codes #&#39; @inheritParams check_weights #&#39; @param .weights_to (optional) new column name for storing weights that will be applied to. The default name is `split_&lt;&lt;code_in&gt;&gt;`. #&#39; input values. #&#39; #&#39; @return Returns panel map as tibble #&#39; @export #&#39; #&#39; @examples make_pm_equal &lt;- function(code_dict, code_in, code_out, .weights_to = NULL){ ## check and remove for duplicates n_dups &lt;- sum(duplicated(code_dict)) no_dup_links &lt;- n_dups == 0 if (!no_dup_links) { message(&quot;Removing duplicate code_in/code_out rows&quot;) code_dict &lt;- code_dict |&gt; dplyr::distinct({{code_in}}, {{code_out}}) } ## make column name for weights .weights_to &lt;- .weights_to %||% paste(&quot;split&quot;, deparse(substitute(code_in)), sep = &quot;_&quot;) ## make panel map panel_map &lt;- code_dict |&gt; dplyr::group_by({{code_in}}) |&gt; dplyr::mutate(n_dest = dplyr::n(), !!.weights_to := 1 / n_dest) |&gt; dplyr::ungroup() |&gt; dplyr::select(-n_dest) return(panel_map) } #&#39; @rdname make_pm_equal #&#39; @export make_panel_map_equal &lt;- make_pm_equal Use this helper on the concordance table defined above: make_pm_equal(codes_BA, std_A, std_B, &quot;weights&quot;) ## # A tibble: 10 × 3 ## std_B std_A weights ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 A1 x1111 1 ## 2 B2 x2222 1 ## 3 B2 x3333 1 ## 4 C3 x4444 0.25 ## 5 C4 x4444 0.25 ## 6 C4 x6666 1 ## 7 C5 x4444 0.25 ## 8 C6 x4444 0.25 ## 9 C7 x5555 0.5 ## 10 C8 x5555 0.5 This function uses the no_dup_links flag to removes any duplicate instructions/links, to avoid assigning unequal shares to each target code/category (shown as naive_share): library(dplyr) codes &lt;- tribble(~code_in, ~code_out, &quot;cake&quot;, &quot;piece_01&quot;, &quot;cake&quot;, &quot;piece_02&quot;, &quot;cake&quot;, &quot;piece_03&quot;, &quot;cake&quot;, &quot;piece_03&quot; ## duplicated row ) codes |&gt; ## equal share by code_out mutate(equal_share = 1 / n_distinct(code_out)) |&gt; ## without duplicates removed group_by(code_in) |&gt; mutate(&quot;n_dest&quot; = n(), weight := 1 / n_dest) |&gt; ungroup() |&gt; select(-n_dest) |&gt; group_by(code_out) |&gt; summarise( weights = paste(weight, collapse = &quot;+&quot;), naive_share = sum(weight), equal_share = unique(equal_share) ) ## # A tibble: 3 × 4 ## code_out weights naive_share equal_share ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 piece_01 0.25 0.25 0.333 ## 2 piece_02 0.25 0.25 0.333 ## 3 piece_03 0.25+0.25 0.5 0.333 6.1.1.2 Tests # testthat::test_that( # &quot;make_pm_equal() works&quot;, # { # testthat::expect_identical( # make_pm_equal(equal_pm$codes_BA, std_A, std_B, .weights_to = &quot;weight&quot;), equal_pm$pm_BA) # testthat::expect_no_message( # make_pm_equal(equal_pm$codes_BA, std_A, std_B, .weights_to = &quot;weight&quot;)) # } # ) # testthat::test_that( # &quot;make_pm_equal() handles duplicate link correctly&quot;, # { # dup_codes_BA &lt;- rbind(equal_pm$codes_BA, equal_pm$codes_BA[1, ]) # testthat::expect_message( # make_pm_equal(dup_codes_BA, std_A, std_B) # ) # testthat::expect_identical( # make_pm_equal(dup_codes_BA, std_A, std_B, .weights_to = &quot;weight&quot;), equal_pm$pm_BA # ) # } # ) 6.1.2 Get Bad Weights (TBC) Helper for errors in panel map weights 6.2 Data Preparation for Concordance Transformation Things that are checked for: missing values code_in and value_in columns Probably good practice things, but too much hassle to check, so maybe put in vignette? data_in should only have one row/obs per code_in, ideally data_in has only the code_in and values_in columns… the rest get dropped (like with dplyr::summarise()) "],["workflows-and-extensions.html", "7 Workflows and Extensions 7.1 Related Nomenclature Variables 7.2 Multiple Transformations 7.3 Visualising Panel Map Representations 7.4 Custom Mapping Weights", " 7 Workflows and Extensions 7.1 Related Nomenclature Variables Same Measure Diff Measure Same Classification 1: Already harmonised Could be row-concatenated together Likely different observation units 3: Distinct but related variables Share xmap from_set, but not necessarily weights Diff Classification 2: Ex-post harmonisation Related observational units (e.g same country, different years) Data must be transformed before concatenation 4: Distinct variables No harmonisation possible 7.2 Multiple Transformations 7.2.1 Multi-Step Transformation (WIP) Consider the task of transforming some data from classification A to C, and imagine you only have panel maps for the steps A-to-B and B-to-C. It is straightforward to chain these transformations together. 7.2.1.1 Stylized Code 7.2.1.2 Functions 7.2.1.3 Tests 7.2.2 Multi-Group Transformations (WIP) Consider the workflow where you want to apply the same transformation to multiple subsets of a larger dataframe. For example, trade data grouped by country. 7.2.3 Mutli-Map-Group Transformations (WIP) Consider datasets where you might want to apply variations of the same map to distinct subsets of a larger dataframe. 7.3 Visualising Panel Map Representations See viz-panel-maps for more: 7.3.1 Panel Maps as Graphs pm_BA |&gt; plt_pm_sigmoid(std_A, std_B, weight) + scale_fill_brewer(palette=&quot;RdPu&quot;, direction = -1) 7.4 Custom Mapping Weights Mapping weights could be based on: Reference data – e.g. population share, past/future proportions Domain expertise ??? 7.4.1 Manual Design and Encoding 7.4.2 Use Reference Proportions "],["conclude.html", "8 Conclusion 8.1 Write internal data to package 8.2 Document the package 8.3 Include pacakge README.md", " 8 Conclusion 8.1 Write internal data to package ## defined in 02_basics.Rmd usethis::use_data(equal_pm, internal = TRUE, overwrite = TRUE) ## ✔ Adding &#39;R&#39; to Depends field in DESCRIPTION ## ✔ Saving &#39;equal_pm&#39; to &#39;R/sysdata.rda&#39; 8.2 Document the package When you are done defining the package, it remains to convert the Roxygen to documentation. rm(list = ls()) litr::document() # &lt;-- use instead of devtools::document() ## ℹ Updating conformr documentation ## ℹ Loading conformr ## Warning: [check_weights.R:12] @exports is not a known tag ## Warning: [concord.R:16] @return requires a value ## Warning: [make_pm_equal.R:15] @examples requires a value ## Writing &#39;NAMESPACE&#39; ## Writing &#39;check_coverage.Rd&#39; ## Writing &#39;check_missing.Rd&#39; ## Writing &#39;check_weights.Rd&#39; ## Writing &#39;concord.Rd&#39; ## Writing &#39;conformr-package.Rd&#39; ## Writing &#39;has_bad_weights.Rd&#39; ## Writing &#39;has_coverage.Rd&#39; ## Writing &#39;has_missing.Rd&#39; ## Writing &#39;make_pm_equal.Rd&#39; ## Writing &#39;use_panel_map.Rd&#39; ## Writing &#39;grapes-or-or-grapes.Rd&#39; You can also add some extra things to your package here if you like, such as a README, some vignettes, a pkgdown site, etc. See here for an example of how to do this with litr. 8.3 Include pacakge README.md litr::add_readme(file.path(&quot;..&quot;, &quot;source-files&quot;, &quot;README.Rmd&quot;)) ## ✔ Writing &#39;README.Rmd&#39; ## ✔ Adding &#39;^README\\\\.Rmd$&#39; to &#39;.Rbuildignore&#39; ## • Update &#39;README.Rmd&#39; to include installation instructions. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
